{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n"
      ],
      "metadata": {
        "id": "-oBg-ek3iuWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5909fa29-36aa-418e-d78d-f3a4331927db"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import re\n",
        "import nltk\n",
        "import contractions\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import contractions\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# 2. Apply oversampling only on the training set\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n"
      ],
      "metadata": {
        "id": "QPePOxHyJEx2"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')       # Sentence tokenizer\n",
        "nltk.download('wordnet')     # Lemmatizer data\n",
        "nltk.download('omw-1.4')     # WordNet multilingual support\n",
        "nltk.download('stopwords')   # Stopword list\n",
        "# %%\n",
        "import nltk\n",
        "nltk.download('punkt')       # Sentence tokenizer\n",
        "nltk.download('wordnet')     # Lemmatizer data\n",
        "nltk.download('omw-1.4')     # WordNet multilingual support\n",
        "nltk.download('stopwords')   # Stopword list\n",
        "nltk.download('punkt_tab')   # Punkt tokenizer tab resource\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZMgN1zpC61E",
        "outputId": "3e2161dd-6c16-49bb-9574-d2498a8f80fb"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IaZj-6Oji5cQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844e52e8-3d86-41a1-b9a8-fd82c857e453"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# 🧹 Cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)  # expand contractions\n",
        "    text = re.sub('<.*?>', '', text)  # remove HTML\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove punctuation/numbers\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]  # remove stopwords\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]  # lemmatize\n",
        "    return ' '.join(words)\n"
      ],
      "metadata": {
        "id": "GgVne33Uip0r"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Load data (adjust path)\n",
        "df = pd.read_csv('/content/drive/MyDrive/colab,zip/IMDB Dataset.csv')[:8000]  # you can choose your size\n"
      ],
      "metadata": {
        "id": "UlKq1Mz_lOnI"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your dataframe is df with columns 'review' and 'sentiment'\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Label encode sentiments\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['sentiment'])  # e.g. positive=1, negative=0\n",
        "\n",
        "X = df['cleaned_review']\n",
        "y = df['label']\n"
      ],
      "metadata": {
        "id": "aacIbQglIGE4"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Split the data (stratify to keep class balance in splits)\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.8, random_state=101, stratify=y\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "n3rTtiusuBd8"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "# Convert text to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "max_length = 100  # or choose based on data analysis\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n"
      ],
      "metadata": {
        "id": "nvGLlCWqMR4X"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After tokenizing and padding your train and test data:\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "x_train_oversampled, y_train_oversampled = ros.fit_resample(train_padded, y_train)\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=64),\n",
        "    Bidirectional(GRU(64, dropout=0.4, recurrent_dropout=0.2)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_oversampled, y_train_oversampled,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(test_padded, y_test),\n",
        "    callbacks=[early_stop]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW7ETPNgErmp",
        "outputId": "be820165-c1a2-46bd-a53c-f18b555ef439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 264ms/step - accuracy: 0.5314 - loss: 0.7261 - val_accuracy: 0.7337 - val_loss: 0.6566\n",
            "Epoch 2/10\n",
            "\u001b[1m  3/201\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 206ms/step - accuracy: 0.8368 - loss: 0.3763"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Save model and tokenizer for deployment\n",
        "model.save(\"gru_sentiment_model.keras\")\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(le, f)\n"
      ],
      "metadata": {
        "id": "29QcjRQ8lCof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D9jWscrKlCYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g6_k4mHDipWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XbE-8i8hlB1i"
      }
    }
  ]
}